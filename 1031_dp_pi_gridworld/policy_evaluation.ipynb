{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061b96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Linear System Method (Grid View) ===\n",
      "\n",
      " -0.761  -0.551  -0.142   0.000  \n",
      " -0.865  [WALL]   -0.715   0.000  \n",
      " -0.909  -0.913  -0.877  -0.950 \n"
     ]
    }
   ],
   "source": [
    "from env import GridWorld\n",
    "import numpy as np\n",
    "\n",
    "def solve_linear_system(env):\n",
    "    \"\"\"\n",
    "    선형 연립방정식을 풀어서 가치함수 구하기\n",
    "    (I - γP)v = r\n",
    "    \"\"\"\n",
    "    states = [s for s in env.get_states() if not env.is_terminal(s)]\n",
    "    n_states = len(states)\n",
    "    state_to_idx = {s: i for i, s in enumerate(states)}\n",
    "    \n",
    "    # P 행렬과 r 벡터 구성\n",
    "    P = np.zeros((n_states, n_states))\n",
    "    r = np.zeros(n_states)\n",
    "    \n",
    "    for i, state in enumerate(states):\n",
    "        # Equiprobable policy: π(a|s) = 0.25\n",
    "        for action in env.actions:\n",
    "            next_state = env.get_next_state(state, action)\n",
    "            reward = env.get_reward(state, action)\n",
    "            \n",
    "            prob = 0.25  # equiprobable\n",
    "            \n",
    "            # r 벡터: 즉시 보상의 기댓값\n",
    "            r[i] += prob * reward\n",
    "            \n",
    "            # P 행렬: 전이 확률\n",
    "            if not env.is_terminal(next_state):\n",
    "                j = state_to_idx[next_state]\n",
    "                P[i, j] += prob\n",
    "    \n",
    "    # 선형 시스템 풀기: (I - γP)v = r\n",
    "    I = np.eye(n_states)\n",
    "    A = I - env.gamma * P\n",
    "    v_array = np.linalg.solve(A, r)\n",
    "    \n",
    "    # Dictionary로 변환\n",
    "    V = {}\n",
    "    for i, state in enumerate(states):\n",
    "        V[state] = v_array[i]\n",
    "    \n",
    "    # Terminal states\n",
    "    for terminal in env.terminals:\n",
    "        V[terminal] = 0.0\n",
    "    \n",
    "    return V\n",
    "\n",
    "\n",
    "# 실행\n",
    "env = GridWorld()\n",
    "\n",
    "V_linear = solve_linear_system(env)\n",
    "\n",
    "print(\"\\n=== Linear System Method (Grid View) ===\\n\")\n",
    "for r in range(env.rows - 1, -1, -1):\n",
    "    row_str = \"\"\n",
    "    for c in range(env.cols):\n",
    "        state = (r, c)\n",
    "        if state == env.wall:\n",
    "            row_str += \" [WALL]  \"\n",
    "        elif state in env.terminals:\n",
    "            row_str += \"  0.000  \"\n",
    "        else:\n",
    "            row_str += f\"{V_linear[state]:7.3f} \"\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b186f9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수렴 완료! (iteration: 41)\n",
      "\n",
      "=== Value Function (Grid View) ===\n",
      "\n",
      " -0.761  -0.551  -0.142   0.000  \n",
      " -0.865  [WALL]   -0.715   0.000  \n",
      " -0.909  -0.912  -0.877  -0.950 \n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(env, theta=0.0001, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Equiprobable policy (π(a|s) = 0.25)에 대한 Policy Evaluation\n",
    "    \"\"\"\n",
    "    # Value function 초기화\n",
    "    V = {}\n",
    "    for state in env.get_states():\n",
    "        V[state] = 0.0\n",
    "    \n",
    "    # Iterative Policy Evaluation\n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        \n",
    "        # 모든 상태에 대해 업데이트\n",
    "        for state in env.get_states():\n",
    "            if env.is_terminal(state):\n",
    "                continue  # 터미널 상태는 V=0 유지\n",
    "            \n",
    "            v = V[state]\n",
    "            \n",
    "            # Bellman equation for equiprobable policy\n",
    "            new_v = 0\n",
    "            for action in env.actions:\n",
    "                next_state = env.get_next_state(state, action)\n",
    "                reward = env.get_reward(state, action)\n",
    "                \n",
    "                # π(a|s) = 0.25 (equiprobable)\n",
    "                prob = 0.25\n",
    "                new_v += prob * (reward + env.gamma * V[next_state])\n",
    "            \n",
    "            V[state] = new_v\n",
    "            delta = max(delta, abs(v - new_v))\n",
    "        \n",
    "        # 수렴 체크\n",
    "        if delta < theta:\n",
    "            print(f\"수렴 완료! (iteration: {iteration + 1})\")\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "env = GridWorld()\n",
    "V = policy_evaluation(env)\n",
    "\n",
    "# Grid 형태로 시각화\n",
    "print(\"\\n=== Value Function (Grid View) ===\\n\")\n",
    "for r in range(env.rows - 1, -1, -1):  # 2, 1, 0 순서 (상하반전)\n",
    "    row_str = \"\"\n",
    "    for c in range(env.cols):\n",
    "        state = (r, c)\n",
    "        if state == env.wall:\n",
    "            row_str += \" [WALL]  \"\n",
    "        elif state in env.terminals:\n",
    "            row_str += \"  0.000  \"\n",
    "        else:\n",
    "            row_str += f\"{V[state]:7.3f} \"\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee4b553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수렴 완료! (iteration: 41)\n",
      "\n",
      "=== Q-values for state (0, 0) ===\n",
      "Q((0, 0),    U) =   -0.88\n",
      "Q((0, 0),    D) =   -0.92\n",
      "Q((0, 0),    L) =   -0.92\n",
      "Q((0, 0),    R) =   -0.92\n",
      "\n",
      "=== Q-values for state (0, 1) ===\n",
      "Q((0, 1),    U) =   -0.92\n",
      "Q((0, 1),    D) =   -0.92\n",
      "Q((0, 1),    L) =   -0.92\n",
      "Q((0, 1),    R) =   -0.89\n",
      "\n",
      "=== Q-values for state (0, 2) ===\n",
      "Q((0, 2),    U) =   -0.74\n",
      "Q((0, 2),    D) =   -0.89\n",
      "Q((0, 2),    L) =   -0.92\n",
      "Q((0, 2),    R) =   -0.95\n",
      "\n",
      "=== Q-values for state (0, 3) ===\n",
      "Q((0, 3),    U) =   -1.00\n",
      "Q((0, 3),    D) =   -0.95\n",
      "Q((0, 3),    L) =   -0.89\n",
      "Q((0, 3),    R) =   -0.95\n",
      "\n",
      "=== Q-values for state (1, 0) ===\n",
      "Q((1, 0),    U) =   -0.78\n",
      "Q((1, 0),    D) =   -0.92\n",
      "Q((1, 0),    L) =   -0.88\n",
      "Q((1, 0),    R) =   -0.88\n",
      "\n",
      "=== Q-values for state (1, 2) ===\n",
      "Q((1, 2),    U) =   -0.23\n",
      "Q((1, 2),    D) =   -0.89\n",
      "Q((1, 2),    L) =   -0.74\n",
      "Q((1, 2),    R) =   -1.00\n",
      "\n",
      "=== Q-values for state (1, 3) ===\n",
      "Q((1, 3),    U) =    0.00\n",
      "Q((1, 3),    D) =    0.00\n",
      "Q((1, 3),    L) =    0.00\n",
      "Q((1, 3),    R) =    0.00\n",
      "\n",
      "=== Q-values for state (2, 0) ===\n",
      "Q((2, 0),    U) =   -0.78\n",
      "Q((2, 0),    D) =   -0.88\n",
      "Q((2, 0),    L) =   -0.78\n",
      "Q((2, 0),    R) =   -0.60\n",
      "\n",
      "=== Q-values for state (2, 1) ===\n",
      "Q((2, 1),    U) =   -0.60\n",
      "Q((2, 1),    D) =   -0.60\n",
      "Q((2, 1),    L) =   -0.78\n",
      "Q((2, 1),    R) =   -0.23\n",
      "\n",
      "=== Q-values for state (2, 2) ===\n",
      "Q((2, 2),    U) =   -0.23\n",
      "Q((2, 2),    D) =   -0.74\n",
      "Q((2, 2),    L) =   -0.60\n",
      "Q((2, 2),    R) =    1.00\n",
      "\n",
      "=== Q-values for state (2, 3) ===\n",
      "Q((2, 3),    U) =    0.00\n",
      "Q((2, 3),    D) =    0.00\n",
      "Q((2, 3),    L) =    0.00\n",
      "Q((2, 3),    R) =    0.00\n"
     ]
    }
   ],
   "source": [
    "def compute_q_values(env, V):\n",
    "    \"\"\"\n",
    "    V값으로부터 Q값 계산\n",
    "    Q(s,a) = r(s,a) + γ V(s')\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "    \n",
    "    for state in env.get_states():\n",
    "        if env.is_terminal(state):\n",
    "            # Terminal state는 Q값이 0\n",
    "            for action in env.actions:\n",
    "                Q[(state, action)] = 0.0\n",
    "            continue\n",
    "        \n",
    "        for action in env.actions:\n",
    "            next_state = env.get_next_state(state, action)\n",
    "            reward = env.get_reward(state, action)\n",
    "            \n",
    "            # Q(s,a) = r + γ V(s')\n",
    "            Q[(state, action)] = reward + env.gamma * V[next_state]\n",
    "    \n",
    "    return Q\n",
    "\n",
    "\n",
    "def print_q_values(env, Q, state):\n",
    "    \"\"\"\n",
    "    특정 상태의 Q값들 출력\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Q-values for state {state} ===\")\n",
    "    for action in env.actions:\n",
    "        print(f\"Q({state}, {action:>4s}) = {Q[(state, action)]:7.2f}\")\n",
    "\n",
    "# 실행\n",
    "env = GridWorld()\n",
    "\n",
    "# V값 계산 (iterative 또는 linear system 중 하나 선택)\n",
    "V = policy_evaluation(env)\n",
    "\n",
    "# Q값 계산\n",
    "Q = compute_q_values(env, V)\n",
    "\n",
    "# 특정 상태의 Q값 확인\n",
    "print_q_values(env, Q, (0, 0))  # 시작 위치\n",
    "print_q_values(env, Q, (0, 1))  # (0,1)\n",
    "print_q_values(env, Q, (0, 2))  # (0,1)\n",
    "print_q_values(env, Q, (0, 3))  # 오른쪽 위\n",
    "\n",
    "print_q_values(env, Q, (1, 0))  # 시작 위치\n",
    "print_q_values(env, Q, (1, 2))  # (0,1)\n",
    "print_q_values(env, Q, (1, 3))  # 오른쪽 위\n",
    "\n",
    "print_q_values(env, Q, (2, 0))  # 시작 위치\n",
    "print_q_values(env, Q, (2, 1))  # (0,1)\n",
    "print_q_values(env, Q, (2, 2))  # (0,1)\n",
    "print_q_values(env, Q, (2, 3))  # 오른쪽 위\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991004e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.865"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.25 * (\n",
    "    (-0.1 + 0.9*(-0.761))\n",
    "  + (-0.1 + 0.9*(-0.909))\n",
    "  + (-0.1 + 0.9*(-0.865))\n",
    "  + (-0.1 + 0.9*(-0.865))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357570d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.76"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((-0.78)+(-0.78)+(-0.88)+(-0.60))/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "748a3bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.784"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.1 + 0.9*(-0.76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7bb3bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5275000000000001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([0.23,-0.60,-1.00,-0.74])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
